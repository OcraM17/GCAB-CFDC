# Gated Class-Attention with Cascaded Feature Drift Compensation for Exemplar-free Continual Learning of Vision Transformers
[Marco Cotogni](https://scholar.google.com/citations?user=8PUz5lAAAAAJ&hl=it), [Fei Yang](https://scholar.google.com/citations?hl=it&user=S1gksNwAAAAJ), [Claudio Cusano](https://scholar.google.com/citations?hl=it&user=lhZpU_8AAAAJ), [Andrew D. Bagdanov](https://scholar.google.com/citations?hl=it&user=_Fk4YUcAAAAJ) and [Joost van de Weijer](https://scholar.google.com/citations?hl=it&user=Gsw2iUEAAAAJ)

Code will be released soon. 

In the meanwhile ArXiv version of our [Paper](https://arxiv.org/pdf/2211.12292.pdf) is available.

![Architecture](figs/Architecture.png)
### Requirements
In order to use our code, it is possible to create a conda environment using the requirements.txt file and Python 3.9.

![GCAB](figs/GCAB.png)
### Reference
If you are considering using our code or you want to cite our paper please use:

```
@article{cotogni2022gated,
  author = {Cotogni, Marco and Yang, Fei and Cusano, Claudio and Bagdanov, Andrew D. and van de Weijer, Joost},
  title = {Gated Class-Attention with Cascaded Feature Drift Compensation for Exemplar-free Continual Learning of Vision Transformers}, 
  journal={arXiv preprint arxiv:2211.12292},
  year={2022}
}

```
#### Credits
Our code takes inspiration from:

[DyToX](https://github.com/arthurdouillard/dytox)
[HAT](https://github.com/joansj/hat)
